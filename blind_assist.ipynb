{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from gtts import gTTS\n",
    "import os\n",
    "from playsound import playsound\n",
    "import pyttsx3\n",
    "\n",
    "\n",
    "def read_aloud(text):\n",
    "    # Initialize the TTS engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set properties (optional)\n",
    "    engine.setProperty('rate', 150)    # Speed of speech\n",
    "    engine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Use the engine to say the text\n",
    "    engine.say(text)\n",
    "    \n",
    "    # Wait for the speech to finish\n",
    "    engine.runAndWait()\n",
    "    \n",
    "    \n",
    "# Ensure PyTorch is installed\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    print(\"PyTorch is not installed. Please install it using 'pip install torch'\")\n",
    "    exit()\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to a PIL image\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Process the image and generate a caption\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_length=150, num_beams=5, early_stopping=True) \n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    print(caption)\n",
    "\n",
    "    # Display the frame with the caption\n",
    "    # cv2.putText(frame, caption, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # cv2.imshow('BLIP Real-Time Captioning', frame)\n",
    "\n",
    "    print(caption)\n",
    "    \n",
    "\n",
    "    read_aloud(caption)\n",
    "\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import pyttsx3\n",
    "from googletrans import Translator\n",
    "\n",
    "def read_aloud(text):\n",
    "    # Initialize the TTS engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set properties (optional)\n",
    "    engine.setProperty('rate', 150)    # Speed of speech\n",
    "    engine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Use the engine to say the text\n",
    "    engine.say(text)\n",
    "    \n",
    "    # Wait for the speech to finish\n",
    "    engine.runAndWait()\n",
    "\n",
    "def translate_text(text, target_language=\"hi\"):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, dest=target_language)\n",
    "    return translation.text\n",
    "\n",
    "# Ensure PyTorch is installed\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    print(\"PyTorch is not installed. Please install it using 'pip install torch'\")\n",
    "    exit()\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to a PIL image\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Process the image and generate a caption\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    print(\"Original Caption: \", caption)\n",
    "\n",
    "    # Translate the caption to Hindi\n",
    "    hindi_caption = translate_text(caption, \"hi\")\n",
    "    print(\"Translated Caption: \", hindi_caption)\n",
    "\n",
    "    # Display the frame with the caption (optional)\n",
    "    # cv2.putText(frame, hindi_caption, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # cv2.imshow('BLIP Real-Time Captioning', frame)\n",
    "\n",
    "    # Read aloud the translated caption\n",
    "    read_aloud(hindi_caption)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ea1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deep-translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dffab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import pyttsx3\n",
    "from deep_translator import GoogleTranslator\n",
    "import requests\n",
    "from gtts import gTTS\n",
    "import os\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "def read_aloud(text):\n",
    "    # Initialize the TTS engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set properties (optional)\n",
    "    engine.setProperty('rate', 150)    # Speed of speech\n",
    "    engine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Use the engine to say the text\n",
    "    engine.say(text)\n",
    "    \n",
    "    # Wait for the speech to finish\n",
    "    engine.runAndWait()\n",
    "    \n",
    "def read_aloud_hindi(text):\n",
    "    # Initialize the TTS engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set properties (optional)\n",
    "    engine.setProperty('rate', 150)    # Speed of speech\n",
    "    engine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Get available voices\n",
    "    voices = engine.getProperty('voices')\n",
    "\n",
    "    # Print all available voices\n",
    "    for voice in voices:\n",
    "        print(f\"Voice: {voice.name}, ID: {voice.id}, Languages: {voice.languages}\")\n",
    "\n",
    "    # Attempt to find a Hindi voice\n",
    "    hindi_voice = None\n",
    "    for voice in voices:\n",
    "        if 'hi' in voice.languages or 'Hindi' in voice.name:\n",
    "            hindi_voice = voice.id\n",
    "            break\n",
    "    \n",
    "    if hindi_voice:\n",
    "        engine.setProperty('voice', hindi_voice)\n",
    "    else:\n",
    "        print(\"Hindi voice not found. Using default voice.\")\n",
    "\n",
    "    # Use the engine to say the text\n",
    "    engine.say(text)\n",
    "    \n",
    "    # Wait for the speech to finish\n",
    "    engine.runAndWait()\n",
    "\n",
    "def translate_text(text, target_language=\"hi\"):\n",
    "    translator = GoogleTranslator(source='auto', target=target_language)\n",
    "    translation = translator.translate(text)\n",
    "    return translation\n",
    "\n",
    "# Ensure PyTorch is installed\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    print(\"PyTorch is not installed. Please install it using 'pip install torch'\")\n",
    "    exit()\n",
    "\n",
    "# Load the BLIP model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to a PIL image\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Process the image and generate a caption\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    print(\"Original Caption: \", caption)\n",
    "\n",
    "    # Translate the caption to Hindi\n",
    "    hindi_caption = translate_text(caption, \"hi\")\n",
    "    print(\"Translated Caption: \", hindi_caption)\n",
    "    CHUNK_SIZE = 1024\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/EXAVITQu4vr4xnSDxMaL\"\n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": \"ea3d93d2636da80240545f56f82bd13f\"\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": hindi_caption,\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"voice_settings\": {\n",
    "        \"stability\": 0.5,\n",
    "        \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open('output.mp3', 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(\"Audio file saved as 'output.mp3'\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        \n",
    "\n",
    "    # Display the frame with the caption (optional)\n",
    "    # cv2.putText(frame, hindi_caption, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    # cv2.imshow('BLIP Real-Time Captioning', frame)\n",
    "\n",
    "    # Read aloud the translated caption\n",
    "    read_aloud(caption)\n",
    "    read_aloud_hindi(hindi_caption)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1d3c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice: Microsoft David Desktop - English (United States), ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0, Languages: []\n",
      "Voice: Microsoft Zira Desktop - English (United States), ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0, Languages: []\n",
      "Hindi voice not found. Using default voice.\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "def read_aloud(text):\n",
    "    # Initialize the TTS engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set properties (optional)\n",
    "    engine.setProperty('rate', 150)    # Speed of speech\n",
    "    engine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Get available voices\n",
    "    voices = engine.getProperty('voices')\n",
    "\n",
    "    # Print all available voices\n",
    "    for voice in voices:\n",
    "        print(f\"Voice: {voice.name}, ID: {voice.id}, Languages: {voice.languages}\")\n",
    "\n",
    "    # Attempt to find a Hindi voice\n",
    "    hindi_voice = None\n",
    "    for voice in voices:\n",
    "        if 'hi' in voice.languages or 'Hindi' in voice.name:\n",
    "            hindi_voice = voice.id\n",
    "            break\n",
    "    \n",
    "    if hindi_voice:\n",
    "        engine.setProperty('voice', hindi_voice)\n",
    "    else:\n",
    "        print(\"Hindi voice not found. Using default voice.\")\n",
    "\n",
    "    # Use the engine to say the text\n",
    "    engine.say(text)\n",
    "    \n",
    "    # Wait for the speech to finish\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Example usage\n",
    "text = \"नमस्ते, आप कैसे हैं?\"\n",
    "read_aloud(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e9a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
